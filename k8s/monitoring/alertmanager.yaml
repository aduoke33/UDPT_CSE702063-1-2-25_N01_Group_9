# =====================================================
# ALERTMANAGER CONFIGURATION
# Movie Booking System - Alert Rules
# =====================================================
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-rules
  namespace: movie-booking
  labels:
    app: prometheus
data:
  alert-rules.yml: |
    groups:
      - name: service-alerts
        rules:
          # Service down alerts
          - alert: ServiceDown
            expr: up{job=~".*-service"} == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Service {{ $labels.job }} is down"
              description: "Service {{ $labels.job }} has been down for more than 1 minute"
          
          # High error rate
          - alert: HighErrorRate
            expr: |
              (sum(rate(http_requests_total{status=~"5.."}[5m])) by (service) /
              sum(rate(http_requests_total[5m])) by (service)) > 0.05
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High error rate on {{ $labels.service }}"
              description: "Service {{ $labels.service }} has error rate > 5% for last 5 minutes"
          
          # High response time
          - alert: HighResponseTime
            expr: |
              histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, service)) > 2
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High response time on {{ $labels.service }}"
              description: "95th percentile response time > 2s for {{ $labels.service }}"
      
      - name: database-alerts
        rules:
          # PostgreSQL down
          - alert: PostgresDown
            expr: up{job="postgres"} == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "PostgreSQL is down"
              description: "PostgreSQL database has been down for more than 1 minute"
          
          # High database connections
          - alert: HighDatabaseConnections
            expr: pg_stat_activity_count > 80
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High number of database connections"
              description: "PostgreSQL has {{ $value }} active connections"
          
          # Redis down
          - alert: RedisDown
            expr: up{job="redis"} == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Redis is down"
              description: "Redis cache has been down for more than 1 minute"
          
          # Redis high memory usage
          - alert: RedisHighMemory
            expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.9
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Redis memory usage is high"
              description: "Redis is using {{ $value | humanizePercentage }} of max memory"
      
      - name: kubernetes-alerts
        rules:
          # Pod restart alert
          - alert: PodRestartingFrequently
            expr: increase(kube_pod_container_status_restarts_total[1h]) > 5
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Pod {{ $labels.pod }} is restarting frequently"
              description: "Pod {{ $labels.pod }} has restarted {{ $value }} times in the last hour"
          
          # High CPU usage
          - alert: HighCPUUsage
            expr: |
              (sum(rate(container_cpu_usage_seconds_total{namespace="movie-booking"}[5m])) by (pod) /
              sum(kube_pod_container_resource_limits{resource="cpu", namespace="movie-booking"}) by (pod)) > 0.9
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "High CPU usage on {{ $labels.pod }}"
              description: "Pod {{ $labels.pod }} is using > 90% CPU"
          
          # High memory usage
          - alert: HighMemoryUsage
            expr: |
              (sum(container_memory_usage_bytes{namespace="movie-booking"}) by (pod) /
              sum(kube_pod_container_resource_limits{resource="memory", namespace="movie-booking"}) by (pod)) > 0.9
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "High memory usage on {{ $labels.pod }}"
              description: "Pod {{ $labels.pod }} is using > 90% memory"
          
          # Persistent Volume almost full
          - alert: PersistentVolumeAlmostFull
            expr: |
              (kubelet_volume_stats_used_bytes{namespace="movie-booking"} /
              kubelet_volume_stats_capacity_bytes{namespace="movie-booking"}) > 0.85
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Persistent volume {{ $labels.persistentvolumeclaim }} is almost full"
              description: "PVC {{ $labels.persistentvolumeclaim }} is {{ $value | humanizePercentage }} full"
      
      - name: business-alerts
        rules:
          # High booking failure rate
          - alert: HighBookingFailureRate
            expr: |
              (sum(rate(booking_failures_total[5m])) /
              sum(rate(booking_attempts_total[5m]))) > 0.1
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High booking failure rate"
              description: "Booking failure rate is > 10% for the last 5 minutes"
          
          # Payment processing delays
          - alert: PaymentProcessingDelay
            expr: |
              histogram_quantile(0.95, sum(rate(payment_processing_duration_seconds_bucket[5m])) by (le)) > 10
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Payment processing is slow"
              description: "95th percentile payment processing time > 10s"
          
          # Low available seats percentage (popular movies)
          - alert: LowSeatAvailability
            expr: |
              (sum(showtime_available_seats) by (movie_id) /
              sum(showtime_total_seats) by (movie_id)) < 0.1
            for: 1h
            labels:
              severity: info
            annotations:
              summary: "Low seat availability for movie {{ $labels.movie_id }}"
              description: "Less than 10% seats available for movie {{ $labels.movie_id }}"
---
# Alertmanager Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: movie-booking
  labels:
    app: alertmanager
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m
      smtp_smarthost: 'smtp.gmail.com:587'
      smtp_from: 'alerts@movie-booking.com'
      smtp_require_tls: true
    
    templates:
      - '/etc/alertmanager/templates/*.tmpl'
    
    route:
      receiver: 'default-receiver'
      group_by: ['alertname', 'service']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 4h
      
      routes:
        - receiver: 'critical-receiver'
          match:
            severity: critical
          continue: true
        
        - receiver: 'slack-receiver'
          match:
            severity: warning
          continue: true
    
    receivers:
      - name: 'default-receiver'
        email_configs:
          - to: 'devops@movie-booking.com'
            send_resolved: true
      
      - name: 'critical-receiver'
        email_configs:
          - to: 'oncall@movie-booking.com'
            send_resolved: true
        # Uncomment for PagerDuty integration
        # pagerduty_configs:
        #   - service_key: '<pagerduty-service-key>'
      
      - name: 'slack-receiver'
        # Uncomment for Slack integration
        # slack_configs:
        #   - api_url: '<slack-webhook-url>'
        #     channel: '#alerts'
        #     send_resolved: true
        webhook_configs:
          - url: 'http://notification-service:8000/api/notifications/webhook/alert'
            send_resolved: true
    
    inhibit_rules:
      - source_match:
          severity: 'critical'
        target_match:
          severity: 'warning'
        equal: ['alertname', 'service']
